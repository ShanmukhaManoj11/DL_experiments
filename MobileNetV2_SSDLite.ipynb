{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileNetV2-SSDLite.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN06TZ6CNanECEysG2Ahc3A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShanmukhaManoj11/DL_experiments/blob/master/MobileNetV2_SSDLite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z93H_u-MTKLC",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCdIBwAoTN03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import itertools\n",
        "from math import sqrt\n",
        "import xml.etree.ElementTree as ET"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdcQ31fpTXg4",
        "colab_type": "text"
      },
      "source": [
        "# MobileNetV2-SSDLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NZnIer9iMnU",
        "colab_type": "text"
      },
      "source": [
        "Ref: https://github.com/qfgaohao/pytorch-ssd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fq3wisHU7_O",
        "colab_type": "text"
      },
      "source": [
        "## nn Module utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0wRJm_lU7Tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1, use_bias=False, use_batch_norm=True):\n",
        "  '''\n",
        "  @ combined util to apply conv2d -> batch norm 2d (No activation)\n",
        "  input:\n",
        "    1. in_channels: number of input channels\n",
        "    2. out_channels: number of output channels \n",
        "    3. kernel_size: kernel size for conv2d operation                                                                            \n",
        "    4. stride: stride for conv2d                                                                                                \n",
        "    5. padding: padding value for conv2d operation                                                                              \n",
        "    6. groups: groups value for defining connection between input and output channels                                           \n",
        "               when groups = in_channels, performs depthwise operation                                                         \n",
        "    7. use_bias: boolean variable to specify if bias is needed in the conv2d operation                                          \n",
        "    8. use_batch_norm: boolean variable to specify if batch norm is applied after the conv2d operation                          \n",
        "  '''\n",
        "  if use_batch_norm:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, groups=groups, bias=use_bias),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "  else:\n",
        "    nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, groups=groups, bias=use_bias)\n",
        "\n",
        "def conv2d_bn_relu(in_channels, out_channels, kernel_size, stride, padding, groups=1, use_bias=False, use_batch_norm=True, onnx_compatible=False):\n",
        "  '''\n",
        "  @ combined util to apply conv2d -> batch norm 2d -> relu\n",
        "  input:\n",
        "    1. in_channels: number of input channels\n",
        "    2. out_channels: number of output channels \n",
        "    3. kernel_size: kernel size for conv2d operation                                                                            \n",
        "    4. stride: stride for conv2d                                                                                                \n",
        "    5. padding: padding value for conv2d operation                                                                              \n",
        "    6. groups: groups value for defining connection between input and output channels                                           \n",
        "               when groups = in_channels, performs depthwise operation                                                         \n",
        "    7. use_bias: boolean variable to specify if bias is needed in the conv2d operation                                          \n",
        "    8. use_batch_norm: boolean variable to specify if batch norm is applied after the conv2d operation   \n",
        "    9. onnx_compatible: boolean variable that specifies the use of ReLU or ReLU6                       \n",
        "  '''\n",
        "  ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n",
        "\n",
        "  if use_batch_norm:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, groups=groups, bias=use_bias),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        ReLU(inplace=True)\n",
        "    )\n",
        "  else:\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, groups=groups, bias=use_bias),\n",
        "        ReLU(inplace=True)\n",
        "    )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBHY4hIEJTyI",
        "colab_type": "text"
      },
      "source": [
        "### Separable Convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYWXq6ADJY42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, onnx_compatible=False):\n",
        "  '''\n",
        "  Conv2d as depthwise and pointwise convolution operations\n",
        "  '''\n",
        "  return nn.Sequential(\n",
        "      # depthwise operation\n",
        "      conv2d_bn_relu(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, use_bias=True, use_batch_norm=True, onnx_compatible=onnx_compatible),\n",
        "      # pointwise linear operation\n",
        "      nn.Conv2d(in_channels, out_channels, 1)\n",
        "  )"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcCjrw7khUHU",
        "colab_type": "text"
      },
      "source": [
        "### Inverted residual bottleneck module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnqvOB0ahS5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InvertedResidualBottleneck(nn.Module):\n",
        "  '''\n",
        "  @ Inverted residual bottleneck module - basic block of the mobilenetV2 architecture\n",
        "  '''\n",
        "  def __init__(self, in_channels, out_channels, stride, expansion_ratio, use_batch_norm=True, onnx_compatible=False):\n",
        "    super(InvertedResidualBottleneck, self).__init__()\n",
        "    hidden_channels = round(expansion_ratio*in_channels)\n",
        "    self.residual_connection = (stride==1 and in_channels==out_channels)\n",
        "\n",
        "    if expansion_ratio==1:\n",
        "      self.operation=nn.Sequential(\n",
        "          # depthwise operation: notice groups=in_channels\n",
        "          conv2d_bn_relu(in_channels, in_channels, 3, stride, 1, groups=in_channels, use_bias=False, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible),\n",
        "          # pointwise \"linear\" operation: no ReLU activation\n",
        "          conv2d_bn(in_channels, out_channels, 1, 1, 0, use_bias=False)\n",
        "      )\n",
        "    else:\n",
        "      self.operation=nn.Sequential(\n",
        "          # pointwise operation for expansion based on expansion_ratio\n",
        "          conv2d_bn_relu(in_channels, hidden_channels, 1, 1, 0, use_bias=False, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible),\n",
        "          # depthwise operation\n",
        "          conv2d_bn_relu(hidden_channels, hidden_channels, 3, stride, 1, groups=hidden_channels, use_bias=False, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible),\n",
        "          # pointwise \"linear\" operation\n",
        "          conv2d_bn(hidden_channels, out_channels, 1, 1, 0, use_bias=False)\n",
        "      )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    if self.residual_connection:\n",
        "      return x + self.operation(x)\n",
        "    else:\n",
        "      return self.operation(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o1cDoumhqAE",
        "colab_type": "text"
      },
      "source": [
        "## MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VklVsichtTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MobileNetV2(nn.Module):\n",
        "  '''\n",
        "  @ MobileNetV2 architecture\n",
        "  https://arxiv.org/pdf/1801.04381.pdf\n",
        "  '''\n",
        "  def __init__(self, n_classes=1000, width_multiplier=1.0, dropout_ratio=0.2, use_batch_norm=True, onnx_compatible=False):\n",
        "    super(MobileNetV2, self).__init__()\n",
        "    self.n_classes = n_classes\n",
        "    # layout defines the structure of the inverted residual bottleneck blocks\n",
        "    layout = [\n",
        "              # t, c, n, s\n",
        "              [1, 16, 1, 1],\n",
        "              [6, 24, 2, 2],\n",
        "              [6, 32, 3, 2],\n",
        "              [6, 64, 4, 2],\n",
        "              [6, 96, 3, 1],\n",
        "              [6, 160, 3, 2],\n",
        "              [6, 320, 1, 1]\n",
        "    ]\n",
        "\n",
        "    # add first standard conv(3x3)+bn+relu module converting input 3 channels to 32*width_multiplier channels\n",
        "    self.features = [conv2d_bn_relu(3, int(32*width_multiplier), 3, 2, 1, groups=1, use_bias=False, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible)]\n",
        "    # add inverted residual bottleneck modules\n",
        "    in_channels = int(32*width_multiplier)\n",
        "    for t, c, n, s in layout:\n",
        "      out_channels = int(c*width_multiplier)\n",
        "      for i in range(n):\n",
        "        if i==0:\n",
        "          self.features.append(InvertedResidualBottleneck(in_channels, out_channels, s, t, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible))\n",
        "        else:\n",
        "          self.features.append(InvertedResidualBottleneck(in_channels, out_channels, 1, t, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible))\n",
        "        in_channels = out_channels\n",
        "    # add standard conv(1x1)+bn+relu module\n",
        "    out_channels = int(1280*width_multiplier) if width_multiplier>1.0 else 1280\n",
        "    self.features.append(conv2d_bn_relu(in_channels, out_channels, 1, 1, 0, groups=1, use_bias=False, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible))\n",
        "    self.features = nn.ModuleList(self.features)\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Dropout(dropout_ratio),\n",
        "        nn.Conv2d(out_channels, n_classes, 1, stride=1, padding=0)\n",
        "    )\n",
        "\n",
        "    self.initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "    for op in self.features:\n",
        "      x=op(x)\n",
        "    x = nn.AvgPool2d(7)(x)\n",
        "    x = self.classifier(x).view(-1,self.n_classes)\n",
        "    return x\n",
        "  \n",
        "  def initialize_weights(self):\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv2d):\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, sqrt(2.0/n))\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.BatchNorm2d):\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "      elif isinstance(m, nn.Linear):\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data.zero_()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3yY54LrPBuC",
        "colab_type": "text"
      },
      "source": [
        "## SSDLite with MobileNetV2 base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOK68nTzEjAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mobilenetv2_base(width_multiplier=1.0, use_batch_norm=True, onnx_compatible=False):\n",
        "  '''\n",
        "  @ return \"features\" from MobileNetV2\n",
        "  '''\n",
        "  return MobileNetV2(width_multiplier=width_multiplier, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible).features\n",
        "\n",
        "def auxiliary_layers(in_channels):\n",
        "  '''\n",
        "  @ return auxiliary modules to be sequentially added at the end of the base layers\n",
        "  input: in_channels = out_channels of the last layer of the base layers\n",
        "  '''\n",
        "  return nn.ModuleList([\n",
        "      InvertedResidualBottleneck(in_channels, 512, 2, expansion_ratio=0.2),\n",
        "      InvertedResidualBottleneck(512, 256, 2, expansion_ratio=0.25),\n",
        "      InvertedResidualBottleneck(256, 256, 2, expansion_ratio=0.5),\n",
        "      InvertedResidualBottleneck(256, 64, 2, expansion_ratio=0.25)\n",
        "  ])\n",
        "\n",
        "def predictors(base_layers, n_classes=21, onnx_compatible=False):\n",
        "  '''\n",
        "  @ return modules responsible for predicting localization offsets and class confidences as tuple of module lists\n",
        "  With MobileNetV2 as base network for SSD, output 15th (of the base layers) layer's expansion layer and last layer's output are used as feature maps for predictions\n",
        "    along with the auxiliary layers \n",
        "  '''\n",
        "  in_channels1 = base_layers[14].operation[0][0].out_channels\n",
        "  in_channels2 = base_layers[-1][0].out_channels\n",
        "  loc_layers = nn.ModuleList([\n",
        "      SeparableConv2d(in_channels1, 6*4, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(in_channels2, 6*4, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(512, 6*4, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(256, 6*4, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(256, 6*4, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      nn.Conv2d(64, 6*4, 1)\n",
        "  ])\n",
        "\n",
        "  conf_layers = nn.ModuleList([\n",
        "      SeparableConv2d(in_channels1, 6*n_classes, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(in_channels2, 6*n_classes, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(512, 6*n_classes, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(256, 6*n_classes, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      SeparableConv2d(256, 6*n_classes, 3, 1, 1, onnx_compatible=onnx_compatible),\n",
        "      nn.Conv2d(64, 6*n_classes, 1)\n",
        "  ])\n",
        "  return loc_layers, conf_layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAIgtjZc9Jiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SSDLite(nn.Module):\n",
        "  '''\n",
        "  @ SSD network with \"features\" from MobileNetV2 as base layers\n",
        "  '''\n",
        "  def __init__(self, base_layers, auxiliary_layers, loc_layers, conf_layers, n_classes=21):\n",
        "    '''\n",
        "    @ constructor\n",
        "    inputs:\n",
        "      1. base_layers: base layers (features) from MobileNetV2 as nn.ModuleList\n",
        "      2. auxiliary_layers: extra layers sequentially added to last base layer as nn.ModuleList\n",
        "      3. loc_layers: operations responsible for predicting localization offsets as nn.ModuleList\n",
        "      4. conf_layers: operations responsible for predicting class confidences as nn.ModuleList\n",
        "    '''\n",
        "    super(SSDLite, self).__init__()\n",
        "    self.base_layers=base_layers\n",
        "    self.auxiliary_layers=auxiliary_layers\n",
        "    self.loc_layers=loc_layers\n",
        "    self.conf_layers=conf_layers\n",
        "    self.n_classes=n_classes\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    feature maps used for predicitng localization offsets and classes\n",
        "      1. output from expansion of 15th layer from base layers (NOTE: index starts from 0)\n",
        "      2. output from last layer of base layers\n",
        "      3. output from all auxiliary layers\n",
        "    '''\n",
        "    pred_feature_maps=[]\n",
        "    # forward pass on base layers till 15th layer and cache output of layer 15's expansion operation to use for predictions in pred_feature_maps list\n",
        "    for k in range(14):\n",
        "      x=self.base_layers[k](x)\n",
        "    x=self.base_layers[14].operation[0](x)\n",
        "    pred_feature_maps.append(x)\n",
        "    # forward pass on remaining layers of base network and cache last layer output to use for predictions in pred_feature_maps list\n",
        "    for op in self.base_layers[14].operation[1:]:\n",
        "      x=op(x)\n",
        "    for k in range(15, len(self.base_layers)):\n",
        "      x=self.base_layers[k](x)\n",
        "    pred_feature_maps.append(x)\n",
        "    # forward pass on auxiliary layers and cache intermediate layers to be used for prediction in pred_feature_maps list\n",
        "    for aux in self.auxiliary_layers:\n",
        "      x=aux(x)\n",
        "      pred_feature_maps.append(x)\n",
        "    # use cached layers for prediction\n",
        "    loc, conf=[],[]\n",
        "    for feature_map, loc_layer, conf_layer in zip(pred_feature_maps, self.loc_layers, self.conf_layers):\n",
        "      loc.append(loc_layer(feature_map).permute(0,2,3,1).contiguous())\n",
        "      conf.append(conf_layer(feature_map).permute(0,2,3,1).contiguous())\n",
        "    loc=torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "    conf=torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "    return loc.view(loc.size(0),-1,4), conf.view(conf.size(0),-1,self.n_classes)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L5TVfRkPLZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_MobileNetV2_SSDLite(n_classes=21, width_multiplier=1.0, use_batch_norm=True, onnx_compatible=False):\n",
        "  base_layers = mobilenetv2_base(width_multiplier=width_multiplier, use_batch_norm=use_batch_norm, onnx_compatible=onnx_compatible)\n",
        "\n",
        "  in_channels = base_layers[-1][0].out_channels\n",
        "  aux_layers = auxiliary_layers(in_channels)\n",
        "\n",
        "  loc_layers, conf_layers = predictors(base_layers, n_classes=n_classes, onnx_compatible=onnx_compatible)\n",
        "\n",
        "  return SSDLite(base_layers, aux_layers, loc_layers, conf_layers, n_classes=n_classes)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5kM-wqIQY4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssd=build_MobileNetV2_SSDLite()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nc4Hyn-QgGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11ba4ff4-16f5-463f-f07f-0cfe2345fd48"
      },
      "source": [
        "x=torch.rand((2,3,300,300))\n",
        "print(x.shape)\n",
        "loc_pred, conf_pred=ssd(x)\n",
        "print(loc_pred.shape, conf_pred.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 300, 300])\n",
            "torch.Size([2, 3000, 4]) torch.Size([2, 3000, 21])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}